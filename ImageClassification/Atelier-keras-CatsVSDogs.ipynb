{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional & Deep Learning : Image classification  on MNIST dataset.\n",
    "\n",
    "### Résumé\n",
    "\n",
    "\n",
    "Ce tutoriel est en grande partie inspiré du [blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) de François Chollet à l'initiative de [Keras](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Use convolutonal network build image classifier on colour images\n",
    "* Use pre-trained model (VGG/Inception to improve quality of the results)\n",
    "* Fine-Tuned pre-trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning Librairies\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.image as kpi\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.backend as k\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.applications as ka\n",
    "\n",
    "# Visualisaiton des données\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code lines allow you to check if your computer is using CPU or GPU ressources. <br>\n",
    "**Warning** : You won't be able to use GPU if another notebook is open and using GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"GPU\" if \"GPU\" in [k.device_type for k in device_lib.list_local_devices()] else \"CPU\"\n",
    "print(MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in this TP is the `CatsVSDogs`dataset used in a [Kaggle Contest](https://www.kaggle.com/c/dogs-vs-cats) which contains 25.000 images. It is a huge number when you do not have a lot of computation power. \n",
    "\n",
    "As our goal here is to understand behaviour of algorithm an not to achieve the best performances we have created two different subsambles of this dataset which is availble in the *data* directory.\n",
    "\n",
    "* First subsample : 100 cats images and 100 dogs images for training. 40 cats images and 40 dogs images for validation.\n",
    "* Second subsample : 1000 cats images and 1000 dogs images for training. 400 cats images and 400 dogs images for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use some the image generator of keras, that we will used later, we have to organise the dataset so that each data of a same class are within the same folder. \n",
    "\n",
    "Our data are ten organized this way :\n",
    "\n",
    "```\n",
    "data_dir\n",
    "└───subsample/\n",
    "│   └───train/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.0.jpg\n",
    "│   │   │   │   cat.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.0.jpg\n",
    "│   │   │   │   dog.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   └───validation/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/' # chemin d'accès aux données\n",
    "\n",
    "# subsample directory path \n",
    "\n",
    "N_train = 200 #2000 \n",
    "N_val = 80 #800\n",
    "data_dir_sub = data_dir+'subsample_%d_Ntrain_%d_Nval' %(N_train, N_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration des données\n",
    "\n",
    "The `load_img` function allows to load and image as a PIL image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = kpi.load_img(data_dir_sub+'/train/cats/cat.1.jpg')  # this is a PIL image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `img_to_array` génére un `array numpy` a partir d'une image PIL ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = kpi.img_to_array(img)  \n",
    "plt.imshow(x/255, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "The `ImageDataGenerator` `keras`function allow to apply different treatment on the images (transformation, normalisation). This  transformation allow to increase the size of the dataset and to make the classifier more robust.\n",
    "\n",
    "All the possible transformaiton are listed in the documentation of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi.ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = kpi.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The`.flow()` command generate transformed images from an original image and save its in the specified directory.\n",
    "\n",
    "In the following code we produce 8 of these transformed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\")  # this is a PIL image\n",
    "x = kpi.img_to_array(img)  \n",
    "x_ = np.expand_dims(x, axis=0)\n",
    "\n",
    "if not(os.path.isdir(data_dir_sub+\"/preprocessing_example\")):\n",
    "    os.mkdir(data_dir_sub+\"/preprocessing_example\")\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x_, batch_size=1,save_to_dir=data_dir_sub+\"/preprocessing_example\", save_prefix='cat', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i > 7:\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list=[]\n",
    "for f in os.listdir(data_dir_sub+\"/preprocessing_example\"):\n",
    "    X_list.append(kpi.img_to_array(kpi.load_img(data_dir_sub+\"/preprocessing_example/\"+f)))\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "fig.patch.set_alpha(0)\n",
    "ax = fig.add_subplot(3,3,1)\n",
    "ax.imshow(x/255, interpolation=\"nearest\")\n",
    "ax.set_title(\"Image original\")\n",
    "for i,xt in enumerate(X_list):\n",
    "    ax = fig.add_subplot(3,3,i+2)\n",
    "    ax.imshow(xt/255, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Random transformation %d\" %(i+1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cats_transformation.png\", dpi=100, bbox_to_anchor=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Image classification from scratch with a convolutional network\n",
    "\n",
    "We will here build a classifier with a custom architecture of a convolutional network.\n",
    "\n",
    "We first defined epochs and batch_size parameters.\n",
    "\n",
    "* `epochs`: we start with a low number (5-10) in order to check that computing time is reasonable.\n",
    "* `batch_size`:When using keras Generator, size of the batch should be a divider of the size of the sample, otherwise algorithms produce very unstable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "We defined two `ImageDataGenerator` object :\n",
    "\n",
    "* `train_datagen`: for learning, where different transformation are applied as above, in order to pass various example to the model.\n",
    "* `valid_datagen`: for validation, where only rescaling is apply.\n",
    "\n",
    "**Question** Why different transformation are applied for learning and validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images have different dimensions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.0.jpg\"))\n",
    "x_1 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\"))\n",
    "x_0.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is annoying because all images must have the same dimensions to be used in this network. \n",
    "\n",
    "The `flow_from_directory` methods allows to specify an output dimension in which all transformed images will be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 150\n",
    "img_height = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "valid_datagen = kpi.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/train/\",  # this is the target directory\n",
    "        target_size=(img_width, img_height),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/validation/\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "The model we defined is composed of 3 convolution blocks with the follwing form : \n",
    "\n",
    "* A Conv2D layer with 32-3X3 filters and a `Relu` activation function.\n",
    "* A MaxPooling layer with 2X2 window.\n",
    "\n",
    "Followed by \n",
    "\n",
    "\n",
    "* A flatten layer.\n",
    "* A Dense layer with 64 neurons and a Relu activation function.\n",
    "* A Dropout layer with a 50% drop rate.\n",
    "* A Dense layer with 1 neurons and a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = km.Sequential()\n",
    "model_conv.add(kl.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), data_format=\"channels_last\"))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(32, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(64, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model_conv.add(kl.Dense(64))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.Dropout(0.5))\n",
    "model_conv.add(kl.Dense(1))\n",
    "model_conv.add(kl.Activation('sigmoid'))\n",
    "\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our problem here is a two classes classifier we will use the `binary_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training can then be done by using the `fit_generator`function instead of the `fit`function used in the MNIST notebook. This function can be used by passing generator object instead of data to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "model_conv.fit_generator(train_generator, steps_per_epoch=N_train // batch_size, epochs=epochs, \n",
    "                         validation_data=validation_generator,validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_conv_simple_model = te-ts\n",
    "print(\"Learning TIme for %d epochs : %d seconds\"%(epochs,t_learning_conv_simple_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_conv_val = model_conv.evaluate_generator(validation_generator, N_val /batch_size, verbose=1)\n",
    "score_conv_train = model_conv.evaluate_generator(train_generator, N_train / batch_size, verbose=1)\n",
    "te = time.time()\n",
    "t_prediction_conv_simple_model = te-ts\n",
    "print('Train accuracy:', score_conv_train[1])\n",
    "print('Validation accuracy:', score_conv_val[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_conv_simple_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comments the accuracy and loss value for training and validation to the one observe in the last epochs of training. What do you observe? Is this normal.\n",
    "\n",
    "**Q** What can you say about the performance of this model?\n",
    "\n",
    "**Exercice** Add more transformation to the learning generator. Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pre-trained Network\n",
    "\n",
    "We have seen above that the complexity of the data make it difficult to build quickly an effecient classifier from scratch event with a elaborate method as a convolutional network.\n",
    "\n",
    "We will now see that this problem can easily be tackle by using **pre-trained models**. These models are models that are very complex (see image below). They have been trained on very huge amount of image data in order to classify them. \n",
    "\n",
    "The figure below represent a *VGG 16*. This model is composed of *5 convolutional blocks* which allows to build features on the images. The last block is a *fully convolutional block*. This last block can be seen as a simple *MLP model* which is used on the features build by the convolutional block.\n",
    "\n",
    "How this model, designed to solve a different problem that our problem can be helpfull?\n",
    "\n",
    "Here is our two-stage strategy :  \n",
    "1. we will send our data through the 5 convolutional block in order to build features. These block have been trained on a huge amount of data and can then build intelligent features.\n",
    "2. we will build our own MLP classifier designed to solved our CatsVsDogs problem, and we will trained it on the features build on step one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Illustration du réseau\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Build features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download weight of the 5 blocks convolutional layer.\n",
    "\n",
    "We will now download the weight of a VGG16 models that havec been learned on the [image-net](http://www.image-net.org) dataset. The image-net is composed of millions of images for 1000 categories.\n",
    "\n",
    "If it's the first time you use these weights, you will have to download it (it will start automatically) and they will be save in your home \n",
    "`\"~/.keras/models\"`\n",
    "\n",
    "The *include_top* argument of the `VGG16`application allows to precised if we want to use or not the last block (fully-connected later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet')\n",
    "model_VGG16_without_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building features\n",
    "\n",
    "We will now send our data to this model we loaded in order to build our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/train\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)  \n",
    "features_train = model_VGG16_without_top.predict_generator(generator, N_train / batch_size,  verbose = 1)\n",
    "\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/validation\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "features_validation = model_VGG16_without_top.predict_generator(generator, N_val / batch_size,  verbose = 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Building our classifier on top of features\n",
    "\n",
    "We will know build a simple classifier in order to used the previously build features to classify our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "**Exercise** Write this classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classifier_pretrained_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know save the weights of this classifier to be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_fcm_val = model_VGG_fcm.evaluate(features_validation, validation_labels)\n",
    "score_VGG_fcm_train = model_VGG_fcm.evaluate(features_train, train_labels)\n",
    "te = time.time()\n",
    "t_prediction_VGG_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_fcm_train[1])\n",
    "print('Validation accuracy:', score_VGG_fcm_val[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment the performance of this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG_fcm.save_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "We have notably increase the performance of our model with a model thant is really quick. We can continue to try improve our results by modifying the small MLP classifier network we build?\n",
    " \n",
    "But to really improve our model, it would be nice to also change the weight of the previous layer in order to make them fit our problem.\n",
    "This is possible and it's called FineTuning.\n",
    "\n",
    "In this part we will then build a Model which is composed of the 5 convolutonal block of the VGG model (with it weights learning on image net) and the classifier block we build (with the weight that we have learned previously).\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation.\n",
    "\n",
    "We first download the model as done previously.\n",
    "\n",
    "However, the model will be trained on our image, we then have to specify the input_shape of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a classfier model like the one we build above and we load the learned weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = km.Sequential()\n",
    "top_model.add(kl.Flatten(input_shape=model_VGG16_without_top.output_shape[1:]))\n",
    "top_model.add(kl.Dense(64, activation='relu'))\n",
    "top_model.add(kl.Dropout(0.5))\n",
    "top_model.add(kl.Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_model.load_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we assemble these two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model_VGG_LastConv_fcm = km.Model(inputs=model_VGG16_without_top.input, outputs=top_model(model_VGG16_without_top.output))\n",
    "\n",
    "model_VGG_LastConv_fcm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezed block\n",
    "\n",
    "Our model is ready to be fine-tuned! \n",
    "\n",
    "However, as seen above it containes a huge number of parameters that our computer may not handle.\n",
    "\n",
    "We will start by fine-tuned only the last block of convolution and our classifier. \n",
    "\n",
    "This is possible by updating the trainable arguments of the layers that we don't want to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_VGG_LastConv_fcm.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/train/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/validation/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG_LastConv_fcm.compile(loss='binary_crossentropy',\n",
    "              optimizer=ko.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# fine-tune the model\n",
    "ts = time.time()\n",
    "model_VGG_LastConv_fcm.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=N_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_VGG_LastConv_fcm = te-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_LastConv_fcm_val = model_VGG_LastConv_fcm.evaluate_generator(validation_generator, N_val // batch_size, verbose=1)\n",
    "score_VGG_LastConv_fcm_train = model_VGG_LastConv_fcm.evaluate_generator(train_generator, N_train // batch_size, verbose=1)\n",
    "\n",
    "te = time.time()\n",
    "t_prediction_VGG_LastConv_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_LastConv_fcm_val[1])\n",
    "print('Validation accuracy:', score_VGG_LastConv_fcm_train[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_LastConv_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how our trained model perform on the kaggle real test dataset (data/test)\n",
    "\n",
    "**Exercise** Apply the model to this dataset and display results on a sample to check it performs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/test_kaggle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercised\n",
    "\n",
    "Keras Has a lot of pre-trained model\n",
    "\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "\n",
    "Some have a lot more complex architecture like `InceptionV3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Restart the TP by using a different pre-trained model and apply the required modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "865px",
    "left": "0px",
    "right": "1587.01px",
    "top": "106px",
    "width": "213px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
